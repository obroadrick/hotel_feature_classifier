{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO Load into memory our data (we can not do this quite yet, since the script to compute labels is still running)\n",
    "\"\"\" Here are some useful functions to keep in mind, pulled from the in-class exercise on TensorFlow\n",
    "tf_X_train = np.array(X_train, dtype='float32')\n",
    "train_data = tf.data.Dataset.from_tensor_slices((tf_X_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(500).batch(batch_size).prefetch(1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 # each point is either *good* or *bad*\n",
    "num_features = 128 # each point is a 128-dimension vector of floats\n",
    "\n",
    "# Training parameters (we begin by using the same initial training parameters as we used in our first TensorFlow model in class)\n",
    "learning_rate = 0.0001\n",
    "training_steps = 3000\n",
    "batch_size = 25\n",
    "display_step = 100\n",
    "\n",
    "# Network paramters (again, we will begin with the same as was used in class, and we plan to build/change from there)\n",
    "n_hidden_1 = 28 # 1st layer number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "\n",
    "# A random value generator to initialize weights.\n",
    "random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(random_normal([num_features, n_hidden_1])),\n",
    "    'out': tf.Variable(random_normal([n_hidden_1, num_classes])),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.zeros([num_classes])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 128 neurons.\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Apply sigmoid to layer_1 output for non-linearity.\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "#     # Hidden fully connected layer with 256 neurons.\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     # Apply sigmoid to layer_2 output for non-linearity.\n",
    "#     layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Output fully connected layer with a neuron for each class.\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    return tf.nn.sigmoid(out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model.\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 128 neurons.\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Apply sigmoid to layer_1 output for non-linearity.\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "#     # Hidden fully connected layer with 256 neurons.\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     # Apply sigmoid to layer_2 output for non-linearity.\n",
    "#     layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output fully connected layer with a neuron for each class.\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    return tf.nn.sigmoid(out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss/Metric Functions\n",
    "\n",
    "Here we create the **cross_entropy** and **accuracy** functions that take our inputs/outputs to define our loss functions and scoring metrics similar to what we've used in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Clip prediction values to avoid log(0) error.\n",
    "    \n",
    "#     # Compute cross-entropy.\n",
    "#     y_pred = tf.cast(y_pred, tf.float32)\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "#     #return tf.reduce_mean(-tf.math.log(y_pred)*y_true + -tf.math.log(1-y_pred)*(1-y_true))\n",
    "#     return tf.compat.v1.losses.sigmoid_cross_entropy(y_true, y_pred)\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, 1)), dtype=tf.float32)\n",
    "    logits_tf = tf.cast(tf.reshape(y_pred, (-1, 1)), dtype=tf.float32)\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(y_true_tf, logits_tf)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred = tf.math.round(y_pred)\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.cast(y_pred, tf.int64), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Backward Prop\n",
    "While many things need to be manually defined, we can use `tf.GradientTape()` to track trainable variables and determine their gradient in regards to certain calculations/outputs. This means we don't need to manually define our backwards propogation and simply rely on the `GradientTape().gradient(dy, dyx)` functionality.\n",
    " - Note: This can be changed for higher order gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = neural_net(x)\n",
    "        loss = cross_entropy(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    # Compute gradients - d_loss/d_trainable_variables\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our Model\n",
    "Just as we need to manually define our network, we also may need to define our training cycle. Here we run through the following:\n",
    " 1. Getting a batch from our dataset for ***training_steps*** number of times\n",
    " 2. We update the weights/run the optimziation for these inputs\n",
    " 3. We make a prediction, calculate loss, and determine the accuracy\n",
    " 4. repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing the output of our model\n",
    "X_test = np.array(X_test, dtype='float32')\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "p = neural_net(X_test)\n",
    "l = cross_entropy(p, y_test)\n",
    "a = accuracy(p, y_test)\n",
    "print(l,a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
