{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Keras (since Tensorflow 2.0) is an integrated high-level API for developing Neural Networks using the tensorflow backend. This means that with Keras you can define neural networks that are trained and run against tensorflow and can even leverage GPU support. To implement these networks Keras implements a Functional and Sequential modeling design\n",
    "\n",
    "-----\n",
    "\n",
    "## Sequential\n",
    "The sequential design follow a lot of the paradigms we have seen in the GWU NN library. Essentially the output of one layer is fed into the following layer, and the model can be finalized through the **compile** method.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(x, input_dim))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='loss_function')\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "## Functional\n",
    "The functional design is more flexible in the sequential, as multiple workflows (inputs and outputs) can be defined from the same model, as well as having diverging and converging pathways. These models still need to be compiled, but need to be defined beforehand. Given time we may dive deeper into these types of models.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "inputs_a = Input(input_a_dim)\n",
    "ha_1 = Dense(xa)(inputs_a)\n",
    "\n",
    "inputs_b = Input(input_b_dim)\n",
    "hb_1 = Dense(xb)(inputs_b)\n",
    "\n",
    "con = concatenate(inputs=[h_a1, h_b1])\n",
    "hcon_1 = Dense(xh)(con)\n",
    "\n",
    "output = Dense(1, activation=\"activation\")(hcon_1)\n",
    "\n",
    "model = Model(inputs=[input_a_dim, input_b_dim], output = output)\n",
    "model.compile(loss='loss_function')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Keras Model\n",
    "For our first keras model we'll reimplement one of our earlier models using the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_col = 'Survived'\n",
    "x_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "df = pd.read_csv('titanic_data.csv')\n",
    "y = np.array(df[y_col]).reshape(-1, 1)\n",
    "orig_X = df[x_cols]\n",
    "\n",
    "# Lets standardize our features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "stand_X = scaler.fit_transform(orig_X)\n",
    "X = stand_X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Model\n",
    "To create our sequential model we'll need to import a couple of things from Keras:\n",
    " - keras.models.Sequential - Keras's import for creating Sequential models\n",
    " - keras.layers.Dense - This is how we'll create a fully connected layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "seq_run_num = 1\n",
    "func_run_num = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our actual model, we'll more or less replicate the logic we had previoulsy used within the GWU NN library. The only real difference is that our **loss** function will be *binary_crossentropy* instead of log_loss and we'll need to define an **optimizer**. Optimizers are algorithms for enhancing the way that *Gradient Descent* works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim, run=seq_run_num):\n",
    "    \"\"\"Defines a binary classification model for our titanic dataset.\n",
    "    Args:\n",
    "        input_dim (tuple): Size of the input data\n",
    "        \n",
    "    Returns:\n",
    "        Keras.Sequential: A Keras model for performing binary_classification\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(28, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    dtime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = os.path.join(\"logs\", f\"seq_run#{run}-{dtime}\")\n",
    "    run+=1\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    \n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model, tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model defined, we simply need to fit the model to our training data. To do this we need to define the following:\n",
    " - X - Our inputs\n",
    " - y - Our outputs\n",
    " - epochs - How many times we'll loop through our training dataset\n",
    " - batch_size - How many records are used during one batch/gradient descent\n",
    "\n",
    "We can also define a verbosity or **verbose** parameter that defines what kind of output the training will produce:\n",
    " - 0 = Shows nothing\n",
    " - 1 = (Default) Shows how each epoch progresses during training\n",
    " - 2 = Truncated version of 1\n",
    " - 3 = Shows how many epochs have been completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tensorboard_callback = get_model(X_train.shape[1])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=25, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the complied loss function and metrics we can also **evaluate(X, y)** our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 695us/step - loss: 0.4889 - accuracy: 0.7881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4889432191848755, 0.7881355881690979]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Class\n",
    "Implement the same model, but use the functional API and use the **adam** optimizer. Compare the results of this implementation to that of the sequential model using the graph of the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "def func_model(run=seq_run_num):\n",
    "    inputs = Input(X_train.shape[1])\n",
    "    layer_1 = Dense(28,activation='relu')(inputs)\n",
    "\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(layer_1)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss='loss_function')\n",
    "    \n",
    "    dtime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = os.path.join(\"logs\", f\"seq_run#{run}-{dtime}\")\n",
    "    run+=1\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model, tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 21:16:38.560119: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-10-26 21:16:38.560138: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-10-26 21:16:38.560165: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7325 - accuracy: 0.4594 - val_loss: 0.6759 - val_accuracy: 0.6314\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.5846 - val_loss: 0.6287 - val_accuracy: 0.7034\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6096 - accuracy: 0.6975 - val_loss: 0.5940 - val_accuracy: 0.7161\n",
      "Epoch 4/30\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.6058 - accuracy: 0.8400"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 21:16:38.786100: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-10-26 21:16:38.786117: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-10-26 21:16:38.795844: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2021-10-26 21:16:38.796989: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2021-10-26 21:16:38.798085: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38\n",
      "2021-10-26 21:16:38.798575: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38/Marshalls-MacBook-Pro.local.trace.json.gz\n",
      "2021-10-26 21:16:38.800478: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38\n",
      "2021-10-26 21:16:38.800581: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38/Marshalls-MacBook-Pro.local.memory_profile.json.gz\n",
      "2021-10-26 21:16:38.801108: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38Dumped tool data for xplane.pb to logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38/Marshalls-MacBook-Pro.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38/Marshalls-MacBook-Pro.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38/Marshalls-MacBook-Pro.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38/Marshalls-MacBook-Pro.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/seq_run#1-20211026-211638/train/plugins/profile/2021_10_26_21_16_38/Marshalls-MacBook-Pro.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5902 - accuracy: 0.7400 - val_loss: 0.5682 - val_accuracy: 0.7458\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7933 - val_loss: 0.5449 - val_accuracy: 0.7542\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7929 - val_loss: 0.5270 - val_accuracy: 0.7712\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.7763 - val_loss: 0.5108 - val_accuracy: 0.7839\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7682 - val_loss: 0.4989 - val_accuracy: 0.7839\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.8345 - val_loss: 0.4892 - val_accuracy: 0.7924\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4720 - accuracy: 0.7961 - val_loss: 0.4845 - val_accuracy: 0.7881\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.8180 - val_loss: 0.4788 - val_accuracy: 0.7966\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.8002 - val_loss: 0.4743 - val_accuracy: 0.7966\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7998 - val_loss: 0.4718 - val_accuracy: 0.7966\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8103 - val_loss: 0.4689 - val_accuracy: 0.7924\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.8007 - val_loss: 0.4678 - val_accuracy: 0.7924\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8225 - val_loss: 0.4651 - val_accuracy: 0.7966\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4546 - accuracy: 0.7895 - val_loss: 0.4635 - val_accuracy: 0.7966\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.8010 - val_loss: 0.4623 - val_accuracy: 0.7966\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8339 - val_loss: 0.4609 - val_accuracy: 0.7966\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.7990 - val_loss: 0.4598 - val_accuracy: 0.7966\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.7920 - val_loss: 0.4586 - val_accuracy: 0.7966\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.7913 - val_loss: 0.4587 - val_accuracy: 0.7966\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.7964 - val_loss: 0.4586 - val_accuracy: 0.7966\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8138 - val_loss: 0.4573 - val_accuracy: 0.7966\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.7823 - val_loss: 0.4567 - val_accuracy: 0.7966\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.8210 - val_loss: 0.4563 - val_accuracy: 0.7966\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8086 - val_loss: 0.4567 - val_accuracy: 0.8008\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8104 - val_loss: 0.4558 - val_accuracy: 0.7966\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8152 - val_loss: 0.4563 - val_accuracy: 0.7924\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8079 - val_loss: 0.4561 - val_accuracy: 0.7881\n"
     ]
    }
   ],
   "source": [
    "m, callback = func_model()\n",
    "history = m.fit(X_train, y_train, epochs=30, batch_size=25, validation_data=(X_test, y_test), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(X_train, y_train, epochs=30, batch_size=25, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Model\n",
    "We'll notice that this isn't too far off from the keras implementation. However, it's worth reviewing as we'll be continuing forward relying more heavily on TensorFlow directly rather than Keras.\n",
    "\n",
    "Material sourced from - [aymericdamien github](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network_raw.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset parameters.\n",
    "num_classes = 1 # total classes (0-9 digits).\n",
    "num_features = 7 # data features (img shape: 28*28).\n",
    "\n",
    "# Training parameters.\n",
    "learning_rate = 0.0001\n",
    "training_steps = 3000\n",
    "batch_size = 25\n",
    "display_step = 100\n",
    "\n",
    "# Network parameters.\n",
    "n_hidden_1 = 28 # 1st layer number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "When using a manual model we may need further tweaking of our data to conform to our needs. Below we're transforming the data into the proper typing, but we're also setting up a `tf.data.Dataset`.\n",
    "\n",
    "The `train_data.repeat().shuffle(500).batch(batch_size).prefetch(1)` line defines a data generator that is constantly repeating a shuffled representation of the data and batching it for the training/testing cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_X_train = np.array(X_train, dtype='float32')\n",
    "train_data = tf.data.Dataset.from_tensor_slices((tf_X_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(500).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Defining Structure\n",
    "If we aren't leveraging the Keras APIs, we'll be required to more explicitly list out the different weight structures and layers that we intend to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "\n",
    "# A random value generator to initialize weights.\n",
    "random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(random_normal([num_features, n_hidden_1])),\n",
    "    'out': tf.Variable(random_normal([n_hidden_1, num_classes])),\n",
    "    #'out': tf.Variable(random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.zeros([num_classes])),\n",
    "    #'out': tf.Variable(tf.zeros([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Flexibility\n",
    "\n",
    "However, with this comes a lot of flexibility in the layer by layer operations.\n",
    "\n",
    "Below we can see a more manual definition of the typical structure we've been creating through numpy alone. The benefit to implementing our networks through this approach vs tools like numpy are:\n",
    " - It provides easy access to an existing library of helper functions\n",
    "   - Notice that I can hook into `tf.nn.relu` and `tf.keras.losses.BinaryCrossentropy`\n",
    " - Tensorflow is able to hook directly into CPU and GPU-based acceleration libraries like CBLAS and CUDA\n",
    " - Automated calculations of gradients\n",
    "   - Notice the use of `tf.GradientTape()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 128 neurons.\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Apply sigmoid to layer_1 output for non-linearity.\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "#     # Hidden fully connected layer with 256 neurons.\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     # Apply sigmoid to layer_2 output for non-linearity.\n",
    "#     layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Output fully connected layer with a neuron for each class.\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    return tf.nn.sigmoid(out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model.\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 128 neurons.\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Apply sigmoid to layer_1 output for non-linearity.\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "#     # Hidden fully connected layer with 256 neurons.\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     # Apply sigmoid to layer_2 output for non-linearity.\n",
    "#     layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output fully connected layer with a neuron for each class.\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    return tf.nn.sigmoid(out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss/Metric Functions\n",
    "\n",
    "Further extending what was done to create our network, we'll need to do some similar things when defining a purely custom model. Here we create the **cross_entropy** and **accuracy** functions that take our inputs/outputs to define our loss functions and scoring metrics similar to what we've used in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Clip prediction values to avoid log(0) error.\n",
    "    \n",
    "#     # Compute cross-entropy.\n",
    "#     y_pred = tf.cast(y_pred, tf.float32)\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "#     #return tf.reduce_mean(-tf.math.log(y_pred)*y_true + -tf.math.log(1-y_pred)*(1-y_true))\n",
    "#     return tf.compat.v1.losses.sigmoid_cross_entropy(y_true, y_pred)\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, 1)), dtype=tf.float32)\n",
    "    logits_tf = tf.cast(tf.reshape(y_pred, (-1, 1)), dtype=tf.float32)\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(y_true_tf, logits_tf)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred = tf.math.round(y_pred)\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.cast(y_pred, tf.int64), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Backward Prop\n",
    "While many things need to be manually defined, we can use `tf.GradientTape()` to track trainable variables and determine their gradient in regards to certain calculations/outputs. This means we don't need to manually define our backwards propogation and simply rely on the `GradientTape().gradient(dy, dyx)` functionality.\n",
    " - Note: This can be changed for higher order gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = neural_net(x)\n",
    "        loss = cross_entropy(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    # Compute gradients - d_loss/d_trainable_variables\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our Model\n",
    "Just as we need to manually define our network, we also may need to define our training cycle. Here we run through the following:\n",
    " 1. Getting a batch from our dataset for ***training_steps*** number of times\n",
    " 2. We update the weights/run the optimziation for these inputs\n",
    " 3. We make a prediction, calculate loss, and determine the accuracy\n",
    " 4. repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100, loss: 0.771598, accuracy: 0.680000\n",
      "step: 200, loss: 0.689500, accuracy: 0.720000\n",
      "step: 300, loss: 0.803874, accuracy: 0.800000\n",
      "step: 400, loss: 0.815437, accuracy: 0.920000\n",
      "step: 500, loss: 0.809721, accuracy: 0.840000\n",
      "step: 600, loss: 0.732772, accuracy: 0.640000\n",
      "step: 700, loss: 0.795862, accuracy: 0.760000\n",
      "step: 800, loss: 0.756262, accuracy: 0.760000\n",
      "step: 900, loss: 0.715056, accuracy: 0.680000\n",
      "step: 1000, loss: 0.723025, accuracy: 0.800000\n",
      "step: 1100, loss: 0.768764, accuracy: 0.800000\n",
      "step: 1200, loss: 0.742401, accuracy: 0.800000\n",
      "step: 1300, loss: 0.765656, accuracy: 0.680000\n",
      "step: 1400, loss: 0.660559, accuracy: 0.920000\n",
      "step: 1500, loss: 0.721527, accuracy: 0.880000\n",
      "step: 1600, loss: 0.690078, accuracy: 0.680000\n",
      "step: 1700, loss: 0.609088, accuracy: 0.760000\n",
      "step: 1800, loss: 0.717613, accuracy: 0.720000\n",
      "step: 1900, loss: 0.666805, accuracy: 0.840000\n",
      "step: 2000, loss: 0.629846, accuracy: 0.600000\n",
      "step: 2100, loss: 0.651369, accuracy: 0.840000\n",
      "step: 2200, loss: 0.633098, accuracy: 0.840000\n",
      "step: 2300, loss: 0.718264, accuracy: 0.760000\n",
      "step: 2400, loss: 0.721134, accuracy: 0.680000\n",
      "step: 2500, loss: 0.644376, accuracy: 0.840000\n",
      "step: 2600, loss: 0.643598, accuracy: 0.720000\n",
      "step: 2700, loss: 0.671081, accuracy: 0.760000\n",
      "step: 2800, loss: 0.657810, accuracy: 0.720000\n",
      "step: 2900, loss: 0.604250, accuracy: 0.840000\n",
      "step: 3000, loss: 0.607744, accuracy: 0.720000\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Class\n",
    "Use the model above to assess the accuracy of the model on the X_test/y_test holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.6418753, shape=(), dtype=float32) tf.Tensor(0.7881356, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Space for work\n",
    "X_test = np.array(X_test, dtype='float32')\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "p = neural_net(X_test)\n",
    "l = cross_entropy(p, y_test)\n",
    "a = accuracy(p, y_test)\n",
    "print(l,a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
